{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSI207-Applications of Optimization",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Linear Regression Model"
      ],
      "metadata": {
        "id": "jWXakUwrjfaN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "breYUOVYjamz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bc5d12a-1e8e-42c3-a452-32d6c68d4d25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">3 7038.49584\n",
            ">4 7023.34487\n",
            ">6 7012.07783\n",
            ">9 6982.79521\n",
            ">10 6976.97027\n",
            ">11 6959.60864\n",
            ">12 6926.49791\n",
            ">15 6894.80596\n",
            ">18 6862.09786\n",
            ">19 6859.43300\n",
            ">22 6830.65859\n",
            ">25 6784.43818\n",
            ">27 6756.06480\n",
            ">28 6729.76387\n",
            ">30 6721.24456\n",
            ">32 6706.61744\n",
            ">33 6703.95959\n",
            ">36 6693.63459\n",
            ">38 6691.25891\n",
            ">39 6681.11121\n",
            ">40 6633.59925\n",
            ">41 6619.08604\n",
            ">42 6590.46688\n",
            ">43 6584.36129\n",
            ">44 6565.15301\n",
            ">48 6547.14141\n",
            ">51 6498.07821\n",
            ">53 6496.91833\n",
            ">55 6464.82227\n",
            ">57 6460.42137\n",
            ">58 6447.73470\n",
            ">60 6447.01060\n",
            ">61 6404.82716\n",
            ">63 6376.19509\n",
            ">66 6350.37033\n",
            ">67 6338.88550\n",
            ">68 6318.74534\n",
            ">69 6312.99841\n",
            ">73 6307.95466\n",
            ">75 6298.02928\n",
            ">77 6271.31628\n",
            ">78 6257.58960\n",
            ">83 6252.62312\n",
            ">85 6242.95580\n",
            ">86 6227.73664\n",
            ">87 6187.76019\n",
            ">88 6155.09560\n",
            ">90 6145.58625\n",
            ">91 6134.45575\n",
            ">92 6133.92594\n",
            ">94 6125.16240\n",
            ">95 6098.49454\n",
            ">96 6053.80407\n",
            ">97 6032.13310\n",
            ">98 6018.03421\n",
            ">99 5967.43068\n",
            ">100 5961.26223\n",
            ">102 5951.31908\n",
            ">103 5950.52131\n",
            ">104 5905.87840\n",
            ">107 5892.28968\n",
            ">110 5865.60784\n",
            ">112 5858.82940\n",
            ">117 5833.22923\n",
            ">118 5811.24058\n",
            ">119 5779.40558\n",
            ">120 5763.08542\n",
            ">125 5726.89455\n",
            ">126 5725.18383\n",
            ">127 5723.80226\n",
            ">128 5699.68017\n",
            ">129 5690.49288\n",
            ">131 5656.51651\n",
            ">133 5642.29332\n",
            ">136 5635.76867\n",
            ">137 5628.03122\n",
            ">139 5598.53710\n",
            ">141 5592.25106\n",
            ">142 5579.23881\n",
            ">147 5556.23988\n",
            ">148 5551.70514\n",
            ">149 5526.60404\n",
            ">150 5511.13812\n",
            ">152 5504.40062\n",
            ">154 5493.10141\n",
            ">155 5478.07520\n",
            ">156 5462.09819\n",
            ">159 5453.48064\n",
            ">160 5431.95780\n",
            ">161 5422.75958\n",
            ">162 5405.92085\n",
            ">164 5384.58927\n",
            ">165 5379.23549\n",
            ">166 5362.55068\n",
            ">174 5332.33656\n",
            ">175 5315.05151\n",
            ">176 5312.23159\n",
            ">177 5307.39182\n",
            ">178 5284.88987\n",
            ">179 5269.68214\n",
            ">181 5253.57947\n",
            ">183 5237.13329\n",
            ">185 5214.72760\n",
            ">186 5188.24905\n",
            ">187 5169.97640\n",
            ">190 5117.33622\n",
            ">192 5115.00665\n",
            ">197 5094.41247\n",
            ">198 5074.97615\n",
            ">199 5063.27513\n",
            ">201 5051.49039\n",
            ">202 5044.79814\n",
            ">204 5033.24968\n",
            ">205 5001.20023\n",
            ">206 4999.69449\n",
            ">208 4956.23217\n",
            ">209 4932.60292\n",
            ">210 4931.30204\n",
            ">212 4905.97113\n",
            ">216 4891.97544\n",
            ">217 4878.72932\n",
            ">219 4840.61133\n",
            ">221 4828.82433\n",
            ">222 4825.16638\n",
            ">226 4807.58122\n",
            ">227 4749.02738\n",
            ">228 4732.29086\n",
            ">230 4711.35663\n",
            ">231 4677.41511\n",
            ">232 4670.76056\n",
            ">234 4670.69090\n",
            ">235 4668.32651\n",
            ">238 4640.46043\n",
            ">239 4629.80400\n",
            ">240 4627.38880\n",
            ">243 4616.91324\n",
            ">247 4595.53323\n",
            ">248 4589.27950\n",
            ">249 4585.40644\n",
            ">250 4566.78877\n",
            ">252 4515.78278\n",
            ">253 4485.99021\n",
            ">255 4481.38081\n",
            ">257 4472.70650\n",
            ">258 4459.18829\n",
            ">261 4454.33160\n",
            ">264 4440.49685\n",
            ">268 4429.40330\n",
            ">269 4428.45474\n",
            ">270 4415.85302\n",
            ">273 4393.47381\n",
            ">275 4370.75263\n",
            ">276 4359.31280\n",
            ">277 4330.26921\n",
            ">279 4304.57849\n",
            ">280 4283.78218\n",
            ">281 4279.74099\n",
            ">283 4262.02492\n",
            ">284 4251.37227\n",
            ">287 4224.64247\n",
            ">289 4202.18611\n",
            ">291 4163.94280\n",
            ">292 4158.07915\n",
            ">293 4152.41144\n",
            ">295 4147.81461\n",
            ">298 4125.85200\n",
            ">299 4099.59020\n",
            ">302 4079.35045\n",
            ">303 4065.77293\n",
            ">307 4061.67867\n",
            ">310 4045.39813\n",
            ">312 4039.04107\n",
            ">315 4032.69729\n",
            ">316 4024.43756\n",
            ">318 3988.68445\n",
            ">319 3985.05532\n",
            ">321 3966.39678\n",
            ">322 3951.80341\n",
            ">324 3925.08858\n",
            ">325 3917.35733\n",
            ">326 3910.57672\n",
            ">331 3905.00591\n",
            ">336 3902.36169\n",
            ">337 3879.43652\n",
            ">339 3871.36304\n",
            ">345 3869.62029\n",
            ">348 3869.45538\n",
            ">350 3863.34116\n",
            ">351 3852.26127\n",
            ">352 3847.83896\n",
            ">354 3846.22409\n",
            ">357 3822.92852\n",
            ">359 3817.56418\n",
            ">362 3800.69323\n",
            ">365 3790.18703\n",
            ">367 3779.10883\n",
            ">368 3760.20364\n",
            ">371 3742.11671\n",
            ">372 3727.03852\n",
            ">373 3715.61787\n",
            ">375 3680.18921\n",
            ">377 3677.03422\n",
            ">379 3656.55659\n",
            ">381 3637.31169\n",
            ">384 3624.69390\n",
            ">385 3601.29051\n",
            ">386 3577.11259\n",
            ">388 3565.89622\n",
            ">389 3556.19204\n",
            ">392 3555.15088\n",
            ">393 3546.73090\n",
            ">397 3540.21736\n",
            ">399 3511.38249\n",
            ">402 3499.60153\n",
            ">403 3491.57442\n",
            ">404 3474.62424\n",
            ">405 3468.32348\n",
            ">406 3463.15369\n",
            ">408 3451.20797\n",
            ">409 3430.41138\n",
            ">410 3406.45176\n",
            ">413 3396.25442\n",
            ">414 3382.25564\n",
            ">418 3373.50347\n",
            ">420 3360.15299\n",
            ">422 3349.62116\n",
            ">423 3343.71960\n",
            ">424 3328.12082\n",
            ">425 3318.69492\n",
            ">427 3292.81512\n",
            ">431 3283.39744\n",
            ">432 3277.14925\n",
            ">433 3270.26389\n",
            ">435 3250.20634\n",
            ">436 3233.18902\n",
            ">437 3207.11264\n",
            ">439 3183.88365\n",
            ">440 3179.45470\n",
            ">444 3166.15146\n",
            ">446 3162.36671\n",
            ">447 3152.33845\n",
            ">452 3126.35972\n",
            ">454 3123.39806\n",
            ">455 3110.96085\n",
            ">457 3102.21581\n",
            ">458 3082.51156\n",
            ">459 3079.34313\n",
            ">461 3062.50314\n",
            ">462 3037.09694\n",
            ">463 3018.98116\n",
            ">465 3018.37367\n",
            ">466 3014.67465\n",
            ">468 3007.22595\n",
            ">472 2999.97398\n",
            ">474 2979.02215\n",
            ">475 2976.93288\n",
            ">476 2962.09886\n",
            ">477 2957.52247\n",
            ">480 2950.91897\n",
            ">484 2939.21169\n",
            ">485 2918.53730\n",
            ">489 2906.21343\n",
            ">492 2894.88960\n",
            ">494 2884.73650\n",
            ">495 2878.11638\n",
            ">496 2869.70958\n",
            ">497 2859.89997\n",
            ">498 2855.19951\n",
            ">499 2824.99128\n",
            ">500 2804.91818\n",
            ">501 2781.93070\n",
            ">503 2778.60979\n",
            ">514 2774.64233\n",
            ">516 2748.56454\n",
            ">518 2725.11572\n",
            ">519 2717.58269\n",
            ">522 2684.16399\n",
            ">526 2668.73195\n",
            ">527 2653.60110\n",
            ">528 2650.99312\n",
            ">530 2647.39880\n",
            ">531 2647.00803\n",
            ">533 2645.88545\n",
            ">538 2630.67128\n",
            ">541 2615.23910\n",
            ">544 2603.05329\n",
            ">547 2600.95776\n",
            ">548 2592.94599\n",
            ">549 2571.99429\n",
            ">550 2563.86432\n",
            ">551 2558.43327\n",
            ">554 2556.04989\n",
            ">556 2545.40932\n",
            ">557 2528.06444\n",
            ">560 2522.52958\n",
            ">561 2509.17320\n",
            ">565 2502.38631\n",
            ">567 2501.50972\n",
            ">569 2500.10456\n",
            ">572 2494.97656\n",
            ">573 2481.97824\n",
            ">574 2474.39429\n",
            ">576 2473.52862\n",
            ">577 2458.46685\n",
            ">579 2457.17126\n",
            ">580 2438.00894\n",
            ">581 2428.14799\n",
            ">583 2417.26282\n",
            ">586 2392.08798\n",
            ">589 2370.97911\n",
            ">591 2345.81793\n",
            ">592 2338.04232\n",
            ">593 2330.65977\n",
            ">595 2315.37920\n",
            ">598 2312.83819\n",
            ">599 2294.98703\n",
            ">600 2282.74144\n",
            ">602 2267.04498\n",
            ">605 2266.33387\n",
            ">610 2259.73726\n",
            ">612 2234.47768\n",
            ">613 2228.05982\n",
            ">614 2227.38663\n",
            ">616 2212.27994\n",
            ">617 2190.57525\n",
            ">619 2181.15161\n",
            ">620 2178.02983\n",
            ">622 2166.74094\n",
            ">623 2162.91402\n",
            ">624 2160.45565\n",
            ">625 2159.62251\n",
            ">626 2156.85659\n",
            ">627 2151.69041\n",
            ">628 2144.66570\n",
            ">630 2138.36345\n",
            ">634 2130.36216\n",
            ">635 2109.36153\n",
            ">639 2085.18248\n",
            ">640 2074.88295\n",
            ">641 2056.62670\n",
            ">642 2051.63951\n",
            ">645 2040.04238\n",
            ">649 2029.49859\n",
            ">653 1995.80557\n",
            ">654 1981.40671\n",
            ">656 1971.34645\n",
            ">659 1965.92534\n",
            ">661 1958.70722\n",
            ">662 1953.97952\n",
            ">663 1953.80466\n",
            ">665 1944.69610\n",
            ">666 1942.21347\n",
            ">669 1934.53345\n",
            ">670 1916.88752\n",
            ">671 1898.08127\n",
            ">674 1888.63115\n",
            ">675 1886.89112\n",
            ">676 1860.26715\n",
            ">677 1842.10293\n",
            ">680 1815.09019\n",
            ">681 1807.04190\n",
            ">683 1800.72330\n",
            ">684 1792.83698\n",
            ">685 1770.05373\n",
            ">688 1760.86462\n",
            ">689 1744.79230\n",
            ">690 1734.21679\n",
            ">691 1726.43163\n",
            ">692 1722.81147\n",
            ">694 1716.99110\n",
            ">695 1709.19934\n",
            ">697 1697.30756\n",
            ">698 1683.41385\n",
            ">699 1675.30568\n",
            ">700 1666.54106\n",
            ">702 1653.34755\n",
            ">710 1643.33289\n",
            ">711 1626.42139\n",
            ">712 1604.59554\n",
            ">713 1594.24030\n",
            ">715 1582.68015\n",
            ">716 1576.44702\n",
            ">718 1575.21637\n",
            ">721 1574.37123\n",
            ">722 1556.06645\n",
            ">725 1555.47691\n",
            ">728 1552.06379\n",
            ">732 1543.69099\n",
            ">734 1530.74440\n",
            ">735 1520.20409\n",
            ">736 1507.23409\n",
            ">737 1491.95412\n",
            ">738 1471.88751\n",
            ">740 1463.44416\n",
            ">748 1458.99602\n",
            ">749 1455.29227\n",
            ">750 1454.23738\n",
            ">754 1450.91714\n",
            ">757 1424.58600\n",
            ">758 1413.97547\n",
            ">761 1413.16155\n",
            ">762 1388.82151\n",
            ">763 1383.81136\n",
            ">765 1383.58957\n",
            ">768 1373.40904\n",
            ">771 1350.96254\n",
            ">772 1341.34619\n",
            ">773 1332.70327\n",
            ">774 1328.32934\n",
            ">777 1315.22337\n",
            ">778 1303.95102\n",
            ">779 1293.82086\n",
            ">780 1288.28298\n",
            ">781 1269.85166\n",
            ">783 1264.30673\n",
            ">784 1262.03853\n",
            ">791 1251.65332\n",
            ">792 1244.71839\n",
            ">796 1241.92567\n",
            ">799 1238.68140\n",
            ">805 1232.67170\n",
            ">806 1227.11762\n",
            ">809 1226.63363\n",
            ">811 1220.06267\n",
            ">812 1203.17849\n",
            ">814 1202.88472\n",
            ">821 1192.75917\n",
            ">822 1181.80550\n",
            ">826 1169.04270\n",
            ">829 1166.18454\n",
            ">830 1157.36400\n",
            ">834 1153.50606\n",
            ">836 1135.55271\n",
            ">840 1127.36237\n",
            ">842 1123.62907\n",
            ">844 1104.66464\n",
            ">847 1097.37668\n",
            ">851 1091.00971\n",
            ">853 1082.96333\n",
            ">854 1079.38039\n",
            ">856 1071.51994\n",
            ">861 1047.39199\n",
            ">863 1044.28639\n",
            ">864 1034.40788\n",
            ">866 1028.63442\n",
            ">867 1023.14877\n",
            ">868 1014.97497\n",
            ">869 1010.23613\n",
            ">872 994.95883\n",
            ">873 981.41002\n",
            ">874 965.41337\n",
            ">875 961.77593\n",
            ">876 960.88817\n",
            ">877 960.41924\n",
            ">878 960.30503\n",
            ">879 945.53663\n",
            ">882 935.31296\n",
            ">884 927.75827\n",
            ">886 922.20942\n",
            ">887 915.41134\n",
            ">890 909.17576\n",
            ">896 902.77229\n",
            ">897 895.09210\n",
            ">900 889.70426\n",
            ">901 882.63523\n",
            ">904 873.83751\n",
            ">906 873.56683\n",
            ">908 864.91556\n",
            ">909 863.96842\n",
            ">911 859.94033\n",
            ">917 850.74090\n",
            ">918 848.91968\n",
            ">920 839.97829\n",
            ">922 833.18581\n",
            ">923 821.93250\n",
            ">924 813.35284\n",
            ">926 804.54872\n",
            ">928 798.42845\n",
            ">929 797.26347\n",
            ">931 783.23029\n",
            ">932 767.95024\n",
            ">933 758.31281\n",
            ">934 746.18082\n",
            ">937 741.66481\n",
            ">943 729.80506\n",
            ">944 725.49147\n",
            ">946 724.55827\n",
            ">947 722.49817\n",
            ">950 715.54767\n",
            ">954 707.04283\n",
            ">956 699.05166\n",
            ">959 692.22256\n",
            ">965 690.89875\n",
            ">966 682.09547\n",
            ">967 680.48528\n",
            ">970 675.48038\n",
            ">976 675.11205\n",
            ">978 668.71663\n",
            ">982 664.93239\n",
            ">983 664.44605\n",
            ">984 655.37994\n",
            ">989 652.59036\n",
            ">992 641.19361\n",
            ">996 633.41191\n",
            ">997 620.91134\n",
            ">998 617.88174\n",
            ">999 610.86718\n",
            ">1002 601.01104\n",
            ">1004 595.62018\n",
            ">1005 590.81150\n",
            ">1007 585.52762\n",
            ">1008 568.91197\n",
            ">1009 556.03384\n",
            ">1010 552.17459\n",
            ">1015 538.92491\n",
            ">1020 533.31369\n",
            ">1025 530.61309\n",
            ">1026 524.36887\n",
            ">1030 518.77377\n",
            ">1031 517.74830\n",
            ">1033 509.82584\n",
            ">1035 505.75480\n",
            ">1038 500.77816\n",
            ">1039 492.79372\n",
            ">1040 489.36500\n",
            ">1041 486.38883\n",
            ">1042 483.00257\n",
            ">1045 480.32580\n",
            ">1048 474.52369\n",
            ">1049 469.54176\n",
            ">1051 463.81944\n",
            ">1052 462.07993\n",
            ">1053 460.31993\n",
            ">1057 459.37436\n",
            ">1061 447.88165\n",
            ">1062 444.33122\n",
            ">1063 438.07326\n",
            ">1067 435.89900\n",
            ">1070 429.11507\n",
            ">1071 419.72197\n",
            ">1072 402.24239\n",
            ">1073 390.01199\n",
            ">1074 386.83557\n",
            ">1079 386.59429\n",
            ">1080 375.65937\n",
            ">1081 374.84994\n",
            ">1083 374.73237\n",
            ">1085 372.80003\n",
            ">1087 367.11714\n",
            ">1088 364.71714\n",
            ">1090 360.30489\n",
            ">1091 350.51263\n",
            ">1092 343.92565\n",
            ">1094 342.22676\n",
            ">1096 337.18118\n",
            ">1098 336.83673\n",
            ">1099 332.92179\n",
            ">1100 327.09292\n",
            ">1101 325.93203\n",
            ">1102 322.89096\n",
            ">1105 315.81179\n",
            ">1106 307.87126\n",
            ">1108 304.84782\n",
            ">1109 303.95243\n",
            ">1111 299.18390\n",
            ">1115 298.53955\n",
            ">1117 294.95887\n",
            ">1118 287.58997\n",
            ">1119 284.08085\n",
            ">1120 281.59749\n",
            ">1121 281.08014\n",
            ">1123 275.99296\n",
            ">1125 274.62150\n",
            ">1128 270.03231\n",
            ">1129 269.84080\n",
            ">1132 261.30291\n",
            ">1134 253.15939\n",
            ">1137 252.66349\n",
            ">1138 249.05838\n",
            ">1142 246.62892\n",
            ">1144 235.57767\n",
            ">1148 233.70216\n",
            ">1149 225.69923\n",
            ">1151 224.10868\n",
            ">1153 220.44997\n",
            ">1154 211.31224\n",
            ">1155 210.33212\n",
            ">1157 207.41401\n",
            ">1160 205.51471\n",
            ">1161 196.33920\n",
            ">1168 195.61990\n",
            ">1170 194.97845\n",
            ">1171 193.27165\n",
            ">1175 189.88736\n",
            ">1176 188.91883\n",
            ">1177 184.74171\n",
            ">1180 180.13458\n",
            ">1181 175.79274\n",
            ">1185 174.98842\n",
            ">1186 174.37549\n",
            ">1188 172.93597\n",
            ">1189 172.83135\n",
            ">1194 171.25683\n",
            ">1196 166.89197\n",
            ">1197 163.99491\n",
            ">1202 162.22357\n",
            ">1205 157.46859\n",
            ">1206 156.15541\n",
            ">1210 155.43032\n",
            ">1213 154.71544\n",
            ">1214 149.90311\n",
            ">1216 143.51666\n",
            ">1218 141.38462\n",
            ">1219 139.94054\n",
            ">1221 137.25916\n",
            ">1225 132.69286\n",
            ">1227 129.49843\n",
            ">1229 126.37921\n",
            ">1231 126.01465\n",
            ">1232 125.10859\n",
            ">1234 121.09734\n",
            ">1238 117.26746\n",
            ">1239 115.46220\n",
            ">1240 113.14994\n",
            ">1242 111.32102\n",
            ">1244 107.66549\n",
            ">1245 102.87789\n",
            ">1246 101.07361\n",
            ">1250 98.45622\n",
            ">1252 94.38724\n",
            ">1253 92.90652\n",
            ">1256 89.49022\n",
            ">1258 86.48771\n",
            ">1261 86.11521\n",
            ">1264 83.12391\n",
            ">1265 81.37184\n",
            ">1270 80.05350\n",
            ">1271 76.90251\n",
            ">1272 73.58412\n",
            ">1277 72.49109\n",
            ">1279 72.02874\n",
            ">1281 69.55573\n",
            ">1284 65.59334\n",
            ">1286 61.34121\n",
            ">1288 60.56493\n",
            ">1289 57.89494\n",
            ">1290 57.09926\n",
            ">1291 56.59504\n",
            ">1293 56.50377\n",
            ">1296 54.75082\n",
            ">1298 53.00351\n",
            ">1300 52.46793\n",
            ">1301 49.71464\n",
            ">1302 49.61200\n",
            ">1303 47.27128\n",
            ">1305 43.90899\n",
            ">1306 43.23199\n",
            ">1310 42.38150\n",
            ">1312 40.78206\n",
            ">1315 38.75641\n",
            ">1316 36.49615\n",
            ">1328 34.57817\n",
            ">1329 33.67437\n",
            ">1331 31.02021\n",
            ">1335 29.45247\n",
            ">1336 28.34246\n",
            ">1337 27.49223\n",
            ">1339 26.23116\n",
            ">1341 25.05051\n",
            ">1345 24.20157\n",
            ">1347 23.68134\n",
            ">1349 21.51318\n",
            ">1353 20.00370\n",
            ">1354 19.25513\n",
            ">1355 17.45686\n",
            ">1364 17.08562\n",
            ">1368 16.20627\n",
            ">1377 15.62254\n",
            ">1379 15.51624\n",
            ">1381 15.46197\n",
            ">1383 14.78670\n",
            ">1385 14.27298\n",
            ">1386 12.38090\n",
            ">1389 11.90119\n",
            ">1393 10.42632\n",
            ">1399 9.75892\n",
            ">1400 9.43345\n",
            ">1401 9.24263\n",
            ">1402 9.14396\n",
            ">1405 9.01226\n",
            ">1409 8.62989\n",
            ">1417 8.46731\n",
            ">1418 8.13193\n",
            ">1421 7.84945\n",
            ">1425 7.50936\n",
            ">1429 6.52438\n",
            ">1431 5.99761\n",
            ">1432 4.77051\n",
            ">1436 4.62860\n",
            ">1437 4.59596\n",
            ">1441 4.52253\n",
            ">1443 4.50189\n",
            ">1444 3.64802\n",
            ">1445 3.40468\n",
            ">1447 3.27839\n",
            ">1449 3.11342\n",
            ">1450 3.02183\n",
            ">1453 2.60779\n",
            ">1455 1.77709\n",
            ">1463 1.75149\n",
            ">1465 1.56584\n",
            ">1467 1.42126\n",
            ">1468 1.29031\n",
            ">1470 1.26788\n",
            ">1471 1.03781\n",
            ">1474 0.59226\n",
            ">1480 0.53036\n",
            ">1488 0.38284\n",
            ">1499 0.32232\n",
            ">1502 0.31836\n",
            ">1513 0.31287\n",
            ">1530 0.23037\n",
            ">1532 0.15616\n",
            ">1551 0.14028\n",
            ">1559 0.13631\n",
            ">1697 0.13430\n",
            ">1703 0.09670\n",
            ">1732 0.08615\n",
            "Done!\n",
            "coefficients: [-1.03694992e-01  3.86479469e-02  3.36457189e+00  4.06002389e-02\n",
            "  5.99046537e-02  8.66200238e+01 -5.96926516e-02 -2.23691732e-02\n",
            "  3.64412607e-03 -2.13994171e-03 -1.00261631e-01]\n",
            "Train MSE: 0.086152\n",
            "Test MSE: 0.081999\n"
          ]
        }
      ],
      "source": [
        "from numpy.random import randn\n",
        "from numpy.random import rand\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def predict_row(row, coefficients):\n",
        "  result = coefficients[-1]\n",
        "  for i in range(len(row)):\n",
        "    result += coefficients[i] * row[i]\n",
        "  return result\n",
        "\n",
        "def predict_dataset(x, coefficients):\n",
        "  yhats = list()\n",
        "  for row in x:\n",
        "    yhat = predict_row(row, coefficients)\n",
        "    yhats.append(yhat)\n",
        "  return yhats\n",
        "\n",
        "def objective(x, y, coefficients):\n",
        "  yhat = predict_dataset(x, coefficients)\n",
        "  score = mean_squared_error(y, yhat)\n",
        "  return score\n",
        "\n",
        "def hillclimbing(x, y, objective, solution, n_iter, step_size):\n",
        "  solution_eval = objective(x, y, solution)\n",
        "  for i in range(n_iter):\n",
        "    candidate = solution + randn(len(solution)) * step_size\n",
        "    candidate_eval = objective(x, y, candidate)\n",
        "    if candidate_eval <= solution_eval:\n",
        "      solution, solution_eval = candidate, candidate_eval\n",
        "      print('>%d %.5f' % (i, solution_eval))\n",
        "  return [solution, solution_eval]\n",
        "\n",
        "x, y = make_regression(n_samples=1000, n_features=10, n_informative=2, noise=0.2, random_state=1)\n",
        "\n",
        "x_train, x_test, y_train, y_test, = train_test_split(x, y, test_size=0.33)\n",
        "n_iter = 2000\n",
        "step_size = 0.15\n",
        "n_coef = x.shape[1] + 1\n",
        "solution = rand(n_coef)\n",
        "coefficients, score = hillclimbing(x_train, y_train, objective, solution, n_iter, step_size)\n",
        "print('Done!')\n",
        "print('coefficients: %s' % coefficients)\n",
        "print('Train MSE: %f' % (score) )\n",
        "yhat = predict_dataset(x_test, coefficients)\n",
        "score = mean_squared_error(y_test, yhat)\n",
        "print('Test MSE: %f' % (score))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Logistic Regression Model"
      ],
      "metadata": {
        "id": "kF4D-oYrjme2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random.mtrand import logistic\n",
        "from math import exp\n",
        "from numpy . random import randn\n",
        "from numpy . random import rand \n",
        "from sklearn . datasets import make_classification\n",
        "from sklearn . model_selection import train_test_split\n",
        "from sklearn . metrics import accuracy_score\n",
        "\n",
        "def predict_row(row, coefficients):\n",
        "  result = coefficients[-1]\n",
        "  for i in range(len(row) ):\n",
        "    result += coefficients[i] * row[i]\n",
        "  logistic = 1.0 + exp(-result)\n",
        "  return logistic\n",
        "\n",
        "def predict_dataset(X, coefficients):\n",
        "  yhats = list()\n",
        "  for row in X:\n",
        "    yhat = predict_row(row, coefficients)\n",
        "    yhats . append(yhat)\n",
        "  return yhats\n",
        "\n",
        "def hillclimbing(X, y, objective, solution, n_iter, step_size):\n",
        "  solution_eval = objective(X, y, solution)\n",
        "  for i in range(n_iter):\n",
        "    candidate = solution + randn(len(solution) ) * step_size\n",
        "    candidate_eval = objective(X, y, candidate)\n",
        "    if candidate_eval >= solution_eval :\n",
        "      solution, solution_eval = candidate, candidate_eval\n",
        "      print('>%d %.5f' % (i, solution_eval) )\n",
        "      \n",
        "      return [solution, solution_eval]\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=2, n_redundant=1, random_state=1)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "n_iter = 2000\n",
        "step_size = 0.1\n",
        "n_coef = X.shape[1] + 1\n",
        "solution = rand(n_coef)\n",
        "coefficients, score = hillclimbing(x_train, y_train, objective, solution, n_iter, step_size)\n",
        "print('Done!')\n",
        "print('Coefficients: %s' % coefficients)\n",
        "print('Train Accuracy: %f' % (score) )\n",
        "yhat = predict_dataset(x_test, coefficients)\n",
        "yhat = [round(y) for y in yhat]\n",
        "score = accuracy_score(y_test, yhat)\n",
        "print('Test Accuracy: %f' %(score) )"
      ],
      "metadata": {
        "id": "RbJOgZowjxW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc6597e-9c4a-49c9-e466-729c23b52657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">1 3.68939\n",
            "Done!\n",
            "Coefficients: [ 0.90146822 -0.10288015  0.26257598  0.70742509  0.55571678  0.48127286]\n",
            "Train Accuracy: 3.689392\n",
            "Test Accuracy: 0.166667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Perceptron"
      ],
      "metadata": {
        "id": "jsK-1DdRjm1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import asarray\n",
        "from numpy . random import randn\n",
        "from numpy . random import rand \n",
        "from sklearn . datasets import make_classification\n",
        "from sklearn . model_selection import train_test_split\n",
        "from sklearn . metrics import accuracy_score\n",
        "\n",
        "def transfer(activation):\n",
        "  if activation >= 0.0:\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "def activate(row, weights):\n",
        "  activation = weights[-1]\n",
        "  for i in range(len(row)):\n",
        "    activation += weights[i] * row[i]\n",
        "  return activation\n",
        "\n",
        "def predict_row(row, weights):\n",
        "  activation = activate(row, weights)\n",
        "  return transfer(activation)\n",
        "\n",
        "def predict_dataset(X, weights):\n",
        "  yhats = list()\n",
        "  for row in X:\n",
        "    yhat = predict_row(row, weights)\n",
        "    yhats.append(yhat)\n",
        "  return yhats\n",
        "\n",
        "def objective(X, y, weights):\n",
        "  yhat = predict_dataset(X, weights)\n",
        "  score = accuracy_score(y, yhat)\n",
        "  return score\n",
        "\n",
        "def hillclimbing(X, y, objective, solution, n_iter, step_size):\n",
        "  solution_eval = objective(X, y, solution)\n",
        "  for i in range(n_iter):\n",
        "    candidate = solution + randn(len(solution) ) * step_size\n",
        "    candidate_eval = objective(X, y, candidate)\n",
        "    if candidate_eval >= solution_eval :\n",
        "      solution, solution_eval = candidate, candidate_eval\n",
        "      print('>%d %.5f' % (i, solution_eval) )\n",
        "      \n",
        "      return [solution, solution_eval]\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=2, n_redundant=1, random_state=1)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "n_iter = 1000\n",
        "step_size = 0.05\n",
        "n_weights = X.shape[1] + 1\n",
        "solution = rand(n_weights)\n",
        "weights, score = hillclimbing(x_train, y_train, objective, solution, n_iter, step_size)\n",
        "print('Done!')\n",
        "print('f(%s) = %f' % (weights, score))\n",
        "yhat = predict_dataset(x_test, weights)\n",
        "score = accuracy_score(y_test, yhat)\n",
        "print('Test Accuracy: %.5f' % (score * 100))"
      ],
      "metadata": {
        "id": "wH7UbuOYjyKZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1abe5e5b-2dd0-46f0-e26e-a40fde9a7d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">1 0.50000\n",
            "Done!\n",
            "f([0.76155825 0.92985129 0.56471846 0.6518377  0.69118961 0.62204806]) = 0.500000\n",
            "Test Accuracy: 48.18182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Multilayer Perceptron"
      ],
      "metadata": {
        "id": "VQVcePVRjnC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import exp\n",
        "from numpy.random import randn , rand\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def objective(X,y,network):\n",
        "    yhat = predict_dataset(X,network)\n",
        "    yhat = [round(y) for y in yhat]\n",
        "    score = accuracy_score(y,yhat)\n",
        "    return score\n",
        "\n",
        "# transform function\n",
        "def transfer(activation):\n",
        "    return 1 / (1 + exp(-activation))\n",
        "\n",
        "def activate (row,weights):\n",
        "    activation = weights[-1]\n",
        "    for i in range(len(row)):\n",
        "        activation += weights[i] * row[i]\n",
        "    return activation\n",
        "\n",
        "def predict_row(row,network):\n",
        "    inputs = row\n",
        "    for layer in network:\n",
        "        new_inputs = []\n",
        "        for node in layer:\n",
        "            activation = activate(inputs ,node)\n",
        "            output = transfer(activation)\n",
        "            new_inputs.append(output)\n",
        "        inputs = new_inputs\n",
        "    return inputs[0]\n",
        "\n",
        "def predict_dataset(X,network):\n",
        "    yhats = list()\n",
        "    for row in X:\n",
        "        yhat = predict_row(row,network)\n",
        "        yhats.append(yhat)\n",
        "    return yhats\n",
        "\n",
        "def step(network , step_size):\n",
        "    new_net = list()\n",
        "    for layer in network:\n",
        "        new_layer = list()\n",
        "        for node in layer:\n",
        "            new_node = node.copy() + randn(len(node)) * step_size\n",
        "            new_layer.append(new_node)\n",
        "        new_net.append(new_layer)\n",
        "    return new_net\n",
        "\n",
        "def hillclimbing(X,y,objective,solution, n_iter , step_size):\n",
        "    solution_eval = objective(X,y,solution)\n",
        "    for i in range(n_iter):\n",
        "        candidate = step(solution , step_size)\n",
        "        candidate_eval = objective(X,y,candidate)\n",
        "        if candidate_eval >= solution_eval:\n",
        "            solution , solution_eval = candidate , candidate_eval\n",
        "            print('>%d %f' % (i,solution_eval))\n",
        "    return [solution , solution_eval]\n",
        "\n",
        "# Define dataset\n",
        "X , y = make_classification(n_samples=1000 , n_features=5 , n_informative=2 , n_redundant=1 , random_state=1)\n",
        "# split into train test sets\n",
        "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.33)\n",
        "n_iter = 1000\n",
        "step_size = 0.1\n",
        "n_inputs = X.shape[1]\n",
        "n_hidden = 10\n",
        "hidden1 = [rand(n_inputs+1 ) for _ in range(n_hidden)]\n",
        "output1 = [rand(n_hidden + 1)]\n",
        "network = [hidden1 , output1]\n",
        "network, score = hillclimbing(X_train, y_train, objective, network, n_iter, step_size)\n",
        "print(\"DONE !\")\n",
        "print(\"Best: %f\" % score)\n",
        "#generate predictions for the test dataset\n",
        "yhat = predict_dataset(X_test,network)\n",
        "# round the predictions\n",
        "yhat = [round(y) for y in yhat]\n",
        "# evaluate predictions\n",
        "score = accuracy_score(y_test,yhat)\n",
        "print(\"Accuracy: %.5f\" % (score*100.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNQwWTGxkLUA",
        "outputId": "c964422a-efb0-4baa-fe12-e6641cc667c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">0 0.500000\n",
            ">1 0.500000\n",
            ">2 0.500000\n",
            ">3 0.500000\n",
            ">4 0.500000\n",
            ">5 0.500000\n",
            ">6 0.500000\n",
            ">7 0.500000\n",
            ">8 0.500000\n",
            ">9 0.500000\n",
            ">10 0.500000\n",
            ">11 0.500000\n",
            ">12 0.500000\n",
            ">13 0.502985\n",
            ">14 0.520896\n",
            ">17 0.537313\n",
            ">18 0.541791\n",
            ">19 0.541791\n",
            ">20 0.543284\n",
            ">22 0.546269\n",
            ">23 0.547761\n",
            ">24 0.576119\n",
            ">27 0.610448\n",
            ">29 0.635821\n",
            ">30 0.670149\n",
            ">31 0.685075\n",
            ">33 0.697015\n",
            ">35 0.700000\n",
            ">36 0.700000\n",
            ">39 0.702985\n",
            ">41 0.731343\n",
            ">42 0.741791\n",
            ">45 0.771642\n",
            ">54 0.802985\n",
            ">60 0.813433\n",
            ">61 0.814925\n",
            ">64 0.826866\n",
            ">65 0.831343\n",
            ">69 0.838806\n",
            ">70 0.840299\n",
            ">74 0.844776\n",
            ">75 0.850746\n",
            ">76 0.850746\n",
            ">80 0.864179\n",
            ">115 0.865672\n",
            ">123 0.867164\n",
            ">160 0.868657\n",
            ">161 0.868657\n",
            ">191 0.870149\n",
            ">194 0.873134\n",
            ">273 0.873134\n",
            ">280 0.873134\n",
            ">301 0.873134\n",
            ">326 0.874627\n",
            ">743 0.876119\n",
            ">744 0.876119\n",
            ">891 0.876119\n",
            ">902 0.876119\n",
            "DONE !\n",
            "Best: 0.876119\n",
            "Accuracy: 81.51515\n"
          ]
        }
      ]
    }
  ]
}